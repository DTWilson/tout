---
title: "Three-Outcome clinical trial design (TOut) - research report"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Research_report}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(TOut)
library(ggplot2)
```

## Introduction

This report describes the theory and implementation of methods to design and analyse clinical trials with a three-outcome structure. We will discuss this design in the context of pilot trials assessing the feasibility of a subsequent definitive trial, but the method is general and can equally be applied to trials assessing efficacy.

## Theory

We consider problems where a single-arm pilot trial with sample size $n$ will estimate a statistical parameter $\rho$ and use this to arrive at one of three outcomes. If the estimate lies below a lower threshold, $\hat{\rho} \leq x_0$, the decision is to _stop_. If it lies above an upper threshold, the decision is to _go_ ahead to the definitive trial with a main trial parameter $\rho_m = \rho$. If it lies between these, $x_0 < \hat{\rho} \leq x_1$, we _pause_ and use other information or other stakeholder input to make our decision, which will ultimately be to either _stop_ or _amend-then-go_, where by "amend" we mean make some changes to the intervention or trial design such that the parameter will change by some amount $\tau$, giving a main trial parameter $\rho_m = \rho + \tau$. We typically won't know what $\tau$ is at the design stage, but might be able to specify an interval $\tau \in [\tau_{min}, \tau_{max}]$ which we think is plausible and over which we'd like to control error rates. We will only consider cases where $\tau_{min} \geq 0$.

$$
Decision =
\begin{cases}
 stop    & \hat{\rho} \leq x_0 \\
 pause   & x_0 < \hat{\rho} \leq x_1 \\
 go      & x_1 < \hat{\rho}
 \end{cases}       
$$

To determine optimal values of $n, x_0$ and $x_1$ we introduce the following operating characteristics (OCs):

- $\alpha$: the probability of proceeding to the main trial when $\rho_m \leq \rho_0$;
- $\beta$: the probability of not proceeding to the main trial when $\rho_m \geq \rho_1$; and
- $\gamma$: the probability of not obtaining a _pause_ decision when $\rho = (\rho_0 + \rho_1)/2$.

Since we can make a _go_ decision in two ways, $\alpha$ is the maximum probability of proceeding either directly or following an _amend-then-go_ outcome when $\rho_m \leq\rho_0$. For the latter, we assume there is a constant probability of mistakenly deciding to proceed when in fact $\rho + \tau \leq \rho_0$, and denote this by $\eta$. That is,

$$
\alpha = \max \left[ \max_{\rho \leq \rho_0} Pr(x_1 < \hat{\rho}), \max_{\rho + \tau \leq \rho_0} \eta Pr(x_0 < \hat{\rho} \leq x_1) + Pr(x_1 < \hat{\rho}) \right].
$$
The first term is clearly maximised at $\rho = \rho_0$. The second can be written as

$$
\begin{align}
\eta Pr(x_0 < \hat{\rho} \leq x_1) + Pr(x_1 < \hat{\rho}) &=
\eta Pr(\hat{\rho} \leq x_1) - \eta Pr(\hat{\rho} \leq x_0) + 1 - Pr(\hat{\rho} \leq x_0)\\
&= 1 + (\eta - 1)Pr(\hat{\rho} \leq x_1) - \eta Pr(\hat{\rho} \leq x_0),
\end{align}
$$

and so is maximised at $\rho = \rho_0 - \tau_{min}$.

To control this OC at a nominal level $\alpha^*$, we first take $n$ and $x_1$ as fixed and such that $Pr(\hat{\rho} > x_1 ~|~ \rho = \rho_0) \leq \alpha^*$. Then we set the second term equal to $\alpha^*$ and rearrange to get

$$
Pr(\hat{\rho} \leq x_0) = \frac{1}{\eta} + \frac{\eta - 1}{\eta}Pr(\hat{\rho} \leq x_1) - \frac{\alpha^*}{\eta}.
$$

Using the inverse of $\hat{\rho}$'s distribution function, we can then find the $\x_0$ which gives us $\alpha = \alpha^*$ (or for the binary case, the $x_0$ which maximises $\alpha$ whilst respecting the constraint).

To choose $x_1$, we continue to fix $n$ and choose the largest value such that $x_0 \leq x_1$ (wuith $x_0$ determined using the above procedure) and $\beta \leq \beta^*$. Searching for the largest such $x_1$ will mean finding the largest possible intermediate outcome zone $|x_1 - x_0|$ and thus minimise the third OC $\gamma$. 

An incorrect _stop_ decision may again occur two ways - directly, or following a _pause_ outcome. The OC $\beta$ can therefore be written as

$$
\beta = \max \left[ \max_{\rho > \rho_1} Pr(\hat{\rho} \leq x_0), \max_{\rho + \tau > \rho_1} Pr(\hat{\rho} \leq x_0) + \eta Pr(x_0 < \hat{\rho} \leq x_1) \right],
$$

Where we have assumed the same probability of an incorrect decision following a _pause_ outcome as with the $\alpha$ case. The first term is maximised at $\rho = \rho_1$. The second term can be written as

$$
\eta Pr(\hat{\rho} \leq x_1) + (1 - \eta) Pr(\hat{\rho} \leq x_0),
$$
which is maximised at $\rho = \rho_1 - \tau_{max}$. Since $\tau_{max} > 0$, the second term will never be less than the first and so we can simplify to

$$
\beta = Pr(\hat{\rho} \leq x_0 ~|~ \rho = \rho_1 - \tau_{max}) + \eta Pr(x_0 < \hat{\rho} \leq x_1 ~|~ \rho = \rho_1 - \tau_{max}).
$$
Finally, we can choose $n$ by finding the smallest value such that all constraints are satisfied, when corresponding $x_1$ and $x_0$ are chosen using the above procedure.

Note that this general method can be applied in the special case where no adjustments are going to be made following a _pause_ decision by setting $\tau_{min} = \tau_{max} = 0$.

## Evaluation

For all our evaluations we will use an example problem where the outcome is binary and the hypotheses are $\rho_0 = 0.5, \rho_1 = 0.7$ and we use constraints $\alpha \leq 0.05, \beta \leq 0.2$

### Are three-outcome designs more efficient than two-outcome designs?

One motivation for three-outcome pilot trials is the typically poor precision in pilot parameter estimates and an associated concern that hard _stop_ or _go_ decisions could be made incorrectly too easily. We can shed some light on this by comparing the efficiency of three- and two-outcomes designs in terms of the sample size they require. 

The key parameter to vary here is $\eta$, the probability of making an incorrect decision when we have a pause outcome under the null or alternative hypothesis. When we set $\eta = 0.5$ the optimal three-outcome design reduces to a two-outcome design:

```{r}
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, gamma = 1, eta = 0.5)
```
This makes sense - we don't want to have any chance of a _pause_ outcome because we can't make reliable decisions following it, and we haven't asked for a meaningful constraint on the OC $\gamma$. If we reduce $\eta$, we will see some efficiencies:

```{r}
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, gamma = 1, eta = 0.2)
```
So, we can reduce the sample size from 52 to 41 if we can assume $\eta = 0.2$. This might be quite optimistic; suppose we use $\eta = 0.4$:

```{r}
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, gamma = 1, eta = 0.4)
```
We find that this is barely enough to move away from a two outcome design. Plotting the optimal sample size for a range of $\eta$ gives

```{r}
df <- data.frame(eta = seq(0.0501, 0.5, 0.001))

get_results <- function(z) {
  d <- TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, gamma = 1, eta = z)
  return(c(d$n, d$thresholds))
}

rs <- t(sapply(df$eta, get_results))
df$n <- rs[,1]; df$dif <- rs[,3] - rs[,2]

ggplot(df, aes(eta)) + geom_line(aes(y=n)) +
  geom_line(aes(y = dif), linetype=2) + 
  xlab(expression(paste("Probability of incorrect decision following a pause outcome, ", eta))) +
  ylab("Sample size") +
  theme_minimal()

#ggsave("../paper/figures/eta_ns.pdf", height=9, width=14, units="cm")
#ggsave("../paper/figures/eta_ns.eps", height=9, width=14, units="cm", device = cairo_ps())
```

From this we see the general trend, and can note that for a significant reduction in sample size we need to assume a very strong ability to make the correct progression decisions following a _pause_ outcome. To examine sensitivity to an incorrect assumption of $\eta$, we take each of these optimal designs and find their error rates when $\eta = 0.5$

```{r}
get_true_ocs <- function(z) {
  get_ocs_bin(z[1], z[2], z[3], rho_0 = 0.5, rho_1 = 0.7, tau_min = 0, tau_max = 0, eta = 0.5)[1:2]
}

ocs <- t(apply(rs, 1, get_true_ocs))
df$a <- ocs[,1]; df$b <- ocs[,2]


ggplot(df, aes(eta)) + 
  geom_line(aes(y = a, linetype = "b")) +
  geom_line(aes(y = b, linetype = "c")) + 
  xlab(expression(paste("Probability of incorrect decision following a pause outcome, ", eta))) +
  ylab("Error rate") + 
  scale_linetype_manual(name = "",
                       values = 1:2,
                       labels =c(expression(alpha), expression(beta))) +
  theme_minimal()

#ggsave("../paper/figures/eta_true_ocs.pdf", height=9, width=14, units="cm")
#ggsave("../paper/figures/eta_true_ocs.eps", height=9, width=14, units="cm", device = cairo_ps())
```

For example, compare the sample size saving and error rate inflation resulting from assuming $\eta = 0.2$ when in fact $\eta = 0.5$:

```{r}
df[df$eta == 0.4991,]
df[df$eta == 0.2001,]
```

### Can three-outcome designs facilitate other information feeding into progression decisions?

Another motivation for three-outcome designs is that they can provide a formal space where other information or other stakeholders can contribute to the progression decision in the event of an intermediate or borderline result in the pilot. Assuming $\eta = 0.5$, we can help ensure _pause_ outcome will occur with some desired probability when $\rho$ is at the midpoint of the two hypotheses by constraining the OC $\gamma$. For example, 

```{r}
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, gamma = 0.1)
```

If we want only a 10% chance of direct _go_ or _stop_ decisions when $\rho = (\rho_0 + \rho_1)/2 = 0.6$, we find we need to increase the sample size from $n = 52$ up to $n = 187$. Looking over a range of $\gamma$ constraints gives:

```{r}
df <- data.frame(gamma = seq(0.1, 1, 0.001))

get_results_gamma <- function(z) {
  d <- TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, gamma = z)
  return(c(d$n, d$thresholds))
}

rs <- t(sapply(df$gamma, get_results_gamma))
df$n <- rs[,1]; df$dif <- rs[,3] - rs[,2]

ggplot(df, aes(gamma)) + geom_line(aes(y=n)) +
  geom_line(aes(y = dif), linetype=2) + 
  xlab(expression(paste("Maximum probability of incorrect conclusive decision, ", gamma, "*"))) +
  ylab("Sample size") +
  theme_minimal()

#ggsave("../paper/figures/gamma_ns.pdf", height=9, width=14, units="cm")
#ggsave("../paper/figures/gamma_ns.eps", height=9, width=14, units="cm", device = cairo_ps())
```

For example, to move from $\gamma = 1$ to $\gamma = 0.4$ we need an increase in sample size of:

```{r}
df[df$gamma %in% c(1, 0.4),]
```


### Can three-outcome designs faciliate making adjustments between the pilot and main trials?

Finally, we want to consider how a three-outcome design works when we're planning on making adjustments. First, consider the case where the adjustment effect is known, i.e. when $\tau_{min} = \tau_{max} = \tau$. 

```{r}
df <- data.frame(tau = seq(0, 0.125, 0.0005))
ns <- sapply(df$tau, function(z) TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, tau = c(z, z), max_n = 500, eta = 0.5)$n)
df$n <- ns

ggplot(df, aes(tau)) + geom_line(aes(y=n)) +
  xlab(expression(paste("Known effect of adjustment, ", tau))) +
  ylab("Sample size") +
  theme_minimal()

#ggsave("../paper/figures/tau_ns.pdf", height=9, width=14, units="cm")
#ggsave("../paper/figures/tau_ns.eps", height=9, width=14, units="cm", device = cairo_ps())
```

We see that the larger the known adjustment effect, the more we need to inflate the sample size. This is because when we have $\tau > 0$, the point at which we need to control type II errors shifts left but one of the type I error components stays put at $\rho_0$.

To see the effect on sample size of allowing for a rage for the adjutsment effect $\tau$, we find the optimal $n$ for a range of lower bounds $\tau_{min}$ and interval widths $\tau_{max} - \tau_{min}$:

```{r}
df <- expand.grid(l = seq(0, 0.1, 0.005),
                 w = seq(0, 0.05, 0.005))

ns <- apply(df, 1, function(z) TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, tau = c(z[1], z[1] + z[2]), max_n = 700, eta = 0.5)$n)
df$n <- ns

ggplot(df, aes(l, w)) + geom_tile(aes(fill = n)) +
  scale_fill_continuous(type = "viridis") +
  xlab(expression(paste("Lower bound of adjustment effect, ", tau[min]))) +
  ylab(expression(paste("Width of adjustment effect interval, ", tau[max] - tau[min]))) +
  theme_minimal()

#ggsave("../paper/figures/tau_part_ns.pdf", height=9, width=14, units="cm")
#ggsave("../paper/figures/tau_part_ns.eps", height=9, width=14, units="cm", device = cairo_ps())
```
We find that increasing $\tau_{max}$ while fixing $\tau_{min}$ further increases the sample size required. At the extremes, we find:

```{r}
df[df$l == 0 & df$w == 0,]
df[df$l == 0 & df$w == 0.05,]
df[df$l == 0.1 & df$w == 0,]
df[df$l == 0.1 & df$w == 0.05,]
```

The figure suggests that sample size is being driven primarily by $\tau_{max}$, but there is still some substantial variation due to $\tau_{min}$. For example:

```{r}
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, tau = c(0.1, 0.15), max_n = 8000, eta = 0.5)$n
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, tau = c(0.0, 0.15), max_n = 8000, eta = 0.5)$n

TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, tau = c(0.05, 0.05), max_n = 8000, eta = 0.5)$n
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, tau = c(0.01, 0.05), max_n = 8000, eta = 0.5)$n
```
Finally, we can examine the effect of $\eta$ when allowing adjustments by varying $\eta$ whilst fixing $\tau_{min} = 0.05$ and $\tau_{max} = 0.1$:

```{r}
df <- data.frame(eta = seq(0.0501, 0.5, 0.001))
ns <- sapply(df$eta, function(z) TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.05, beta_nom = 0.1, gamma = 1, eta = z, tau = c(0.05, 0.1), max_n = 500)$n)
df$n <- ns

ggplot(df, aes(eta)) + geom_line(aes(y=n)) +
  #geom_line(aes(y = dif), linetype=2) + 
  xlab(expression(paste("Probability of incorrect decision following a pause outcome, ", eta))) +
  ylab("Sample size") +
  theme_minimal()
```

We see that a lower $\eta$ can give more reasonable sample sizes. Unlike in the non-adjustment case, we might argue $\eta < 0.5$ is plausible because even if we can't learn anything about $\rho$ beyond its sufficient statistic $\hat{rho}$, we might have learnt about $\tau$ from the pilot (i.e. we have a good idea of what adjustment we can make and what effect it will have). But this can only take us so far, since we need to know about the adjusted parameter $\rho + \tau$ so there will always be some residual uncertainty. Thus, extreme cases like the one above where $\eta$ is so low that the optimal design becomes $n=2$ with a almost guaranteed pause result (following which we almost always make the right decision) aren't realistic.

### Relaxing error rate constraints

We have illustrated the impact of three-outcome designs by looking at the changing sample size requirements while keep nominal error rate constraints fixed at values typical for definitive studies. Alternatively, we could fix the sample size at a typical pilot trial level and look at what kind of error rates we can obtain. As an example, consider our problem with $\tau \in [0, 0.1]$ and with $\beta^* = 0.1, \gamma^* = 0.5$. Fixing the sample size to $n = 50$ we find that

```{r}
TOut_design_bin(rho_0 = 0.5, rho_1 = 0.7, alpha_nom = 0.4, beta_nom = 0.1, gamma_nom = 0.5, tau = c(0, 0.05), max_n = 500)
```

## Figures

Sargent sampling distribution illustration:

```{r}
ggplot(data.frame(x = c(-10, 10)), aes(x)) +
  # Null 
  stat_function(fun = dnorm, geom = "line", aes(linetype="a")) +
  stat_function(fun = dnorm, geom = "area", fill = "orange", alpha = 0.4, xlim = c(2, 7)) +
  stat_function(fun = dnorm, geom = "area", fill = "red", alpha = 0.4, xlim = c(1, 2)) +
  
  # Alternative
  stat_function(fun = dnorm, args = list(mean = 2.5), geom = "line", aes(linetype="b")) +
  stat_function(fun = dnorm, args = list(mean = 2.5), 
                geom = "area", fill = "darkgreen", alpha = 0.4, xlim = c(-3, 1)) +
  stat_function(fun = dnorm, args = list(mean = 2.5), 
                geom = "area", fill = "blue", alpha = 0.4, xlim = c(1, 2)) +
  
  geom_label(aes(x = 0.6, y = 0.22, label = "lambda"), parse = T, fill = "red", alpha = 0.4) +
  geom_label(aes(x = 2.4, y = 0.17, label = "delta"), parse = T, fill = "blue", alpha = 0.4) +
  geom_label(aes(x = 0.2, y = 0.1, label = "beta[a]"), parse = T, fill = "darkgreen", alpha = 0.4) +
  geom_label(aes(x = 2.6, y = 0.06, label = "alpha[a]"), parse = T, fill = "orange", alpha = 0.4) +
  
  geom_text(aes(x=1, y=-0.04, label = "x[0]"), parse = T) +
  geom_text(aes(x=2, y=-0.04, label = "x[1]"), parse = T) +
  
  scale_linetype_manual(values = c(1,2), 
                        labels = c(expression(rho[0]), expression(rho[1])), name="") +
  
  xlim(-4, 6.5) + theme_void() + theme(legend.position="bottom")

#ggsave("../paper/figures/Sarg_ocs.pdf", height=4.5, width=14, units="cm")
#ggsave("../paper/figures/Sarg_ocs.eps", height=4.5, width=14, units="cm", device = cairo_ps())
```

Storer sampling distribution illustration:

```{r}
p1 <- ggplot(data.frame(x = c(-10, 10)), aes(x)) +
  # Null 
  stat_function(fun = dnorm, geom = "line", aes(linetype = "a")) +
  stat_function(fun = dnorm, geom = "area", fill = "orange", alpha = 0.4, xlim = c(1, 7)) +
  
  # Alternative
  stat_function(fun = dnorm, args = list(mean = 2.5), geom = "line", aes(linetype = "b")) +
  stat_function(fun = dnorm, args = list(mean = 2.5), 
                geom = "area", fill = "darkgreen", alpha = 0.4, xlim = c(-3, 2)) +
  
  geom_label(aes(x = 0.2, y = 0.12, label = "beta[b]"), parse = T, fill = "darkgreen", alpha = 0.4) +
  geom_label(aes(x = 2.6, y = 0.08, label = "alpha[b]"), parse = T, fill = "orange", alpha = 0.4) +

  geom_text(aes(x=1, y=-0.04, label = "x[0]"), parse = T) +
  geom_text(aes(x=2, y=-0.04, label = "x[1]"), parse = T) +
  
  stat_function(fun = dnorm, geom = "line", aes(linetype = "c"), xlim = c(0,0)) +
  
  scale_linetype_manual(values = c(1, 2, 3), 
                        labels = c(expression(rho[0]), expression(rho[1]),
                                   expression(rho[m])), name="") +
  
  xlim(-4, 6.5) + theme_void() + theme(legend.position="bottom")
```

```{r}
p2 <- ggplot(data.frame(x = c(-10, 10)), aes(x)) +
  # Midpoint
  stat_function(fun = dnorm, args = list(mean = 1.25), geom = "line", linetype=3) +
  stat_function(fun = dnorm, args = list(mean = 1.25), 
                geom = "area", fill = "red", alpha = 0.4, xlim = c(-3, 1)) +
  stat_function(fun = dnorm, args = list(mean = 1.25), 
                geom = "area", fill = "blue", alpha = 0.4, xlim = c(2, 7)) +
  
  geom_label(aes(x = -0.9, y = 0.12, label = "gamma[L]"), parse = T, fill = "red", alpha = 0.4) +
  geom_label(aes(x = 3.4, y = 0.12, label = "gamma[U]"), parse = T, fill = "blue", alpha = 0.4) +
  xlim(-4, 6.5) + theme_void()
```

```{r}
library(patchwork)

p2/p1

#ggsave("../paper/figures/Stor_ocs.pdf", height=8, width=14, units="cm")
#ggsave("../paper/figures/Stor_ocs.eps", height=8, width=14, units="cm", device = cairo_ps())
```